providers:
  test:
    type: openai-compatible
    endpoint: http://localhost:8080/v1

models:
  test/test-model:
    maxTokens: 4096
    contextBudget: 4096

routing:
  default: test/test-model
  fallback: []
  tasks:
    summarization: test/test-model
    subagent: test/test-model

llm:
  maxRetries: 1
  requestTimeoutMs: 5000
  initialBackoffMs: 100
  backoffMultiplier: 2.0

memory:
  embedding:
    type: onnx
    model: all-MiniLM-L6-v2
  chunking:
    size: 512
    overlap: 64
  search:
    topK: 10

context:
  defaultBudgetTokens: 4096
  slidingWindow: 10
  subagentHistory: 5

processing:
  debounceMs: 100
  maxConcurrentLlm: 2
  maxToolCallRounds: 5

logging:
  subagentConversations: false

codeExecution:
  dockerImage: python:3.12-slim
  timeout: 30
  allowNetwork: false
  maxMemory: 256m
  maxCpus: "1.0"
  readOnlyRootfs: true
  keepAlive: false
  keepAliveIdleTimeoutMin: 5
  keepAliveMaxExecutions: 100

files:
  maxFileSizeBytes: 10485760

docs:
  enabled: true
