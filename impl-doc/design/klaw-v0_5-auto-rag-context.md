# Klaw — Изменения v0.5: Auto-RAG и управление контекстом

---

## 1. Auto-RAG по истории сообщений

### Мотивация

В текущем дизайне, когда сообщения покидают sliding window, единственный путь к ним — tool `memory_search`, который ищет по **archival memory** (то что LLM явно сохранил через `memory_save`). Если LLM не вызвал `memory_save` — факт из разговора потерян для поиска, хотя лежит в JSONL и в `klaw.db`.

Типичный сценарий: пользователь обсуждал деплой 100 сообщений назад, тема вышла из окна, LLM не сохранил детали в archival memory. Пользователь спрашивает "что мы решили по деплою?" — LLM не находит ничего ни в окне, ни через `memory_search`.

### Решение: Auto-RAG с предохранителем

Перед каждым вызовом LLM — автоматический поиск по истории сообщений текущего сегмента. Результаты добавляются в контекст между summary и sliding window. **Предохранитель**: если текущий сегмент короче sliding window — поиск пропускается (все сообщения и так в контексте).

```
Сообщение от пользователя
  │
  ▼
Кол-во сообщений в сегменте > slidingWindow?
  │
  нет → пропустить, собрать контекст как обычно
  │
  да → autoRag(user_message, segment, slidingWindowIds)
  │    результаты → добавить в контекст
  ▼
Вызов LLM
```

### Scope: только текущий сегмент

`/new` — осознанное решение пользователя начать с чистого листа. Auto-RAG уважает это и ищет **только в рамках текущего сегмента** (после последнего `/new`). Для доступа к прошлым сегментам и всей истории — LLM вызывает `memory_search` как tool.

Чёткое разделение ответственности:

| Механизм | Scope | Триггер | Назначение |
|----------|-------|---------|------------|
| Auto-RAG | Текущий сегмент | Автоматически, при каждом вызове LLM | Расширенная краткосрочная память текущей сессии |
| `memory_search` tool | Вся archival memory, вся история, все сегменты | LLM вызывает явно | Долгосрочная память агента |

### Реализация поиска

Гибридный поиск по двум индексам, объединённый через RRF:

```
Auto-RAG query (последнее сообщение пользователя)
  │
  ├─→ vec_messages (semantic search, фильтр по segment)
  ├─→ messages_fts (fulltext search, фильтр по segment)
  │
  ▼
RRF (k=60) → фильтрация → topK результатов
```

**Фильтрация результатов:**

1. **Дедупликация**: исключить message_id которые уже в sliding window — не дублировать то что LLM и так видит
2. **Relevance threshold**: если лучший результат хуже порога (distance > `autoRag.relevanceThreshold`) — не включать ничего. "Ок" и "спасибо" не должны тянуть нерелевантные чанки
3. **Budget**: результаты ограничены `autoRag.maxTokens` — жёсткий потолок на расход контекстного бюджета

```kotlin
fun autoRag(
    query: String,
    segmentStart: String,
    slidingWindowIds: Set<String>,
    config: AutoRagConfig
): List<MessageChunk> {
    val queryEmbedding = embed(query)
    
    // Semantic search по vec_messages (только текущий сегмент)
    val vecResults = vecMessagesSearch(queryEmbedding, segmentStart, topK = 20)
    
    // Fulltext search по messages_fts (только текущий сегмент)
    val ftsResults = messagesFtsSearch(query, segmentStart, topK = 20)
    
    // RRF
    val merged = reciprocalRankFusion(vecResults, ftsResults, k = 60)
    
    // Фильтрация
    return merged
        .filter { it.id !in slidingWindowIds }           // дедупликация
        .filter { it.distance <= config.relevanceThreshold } // порог релевантности
        .take(config.topK)
        .truncateToTokenBudget(config.maxTokens)
}
```

### Изменение сборки контекстного окна (секция 3.3)

Было:

```
1. System prompt                    (~500 tokens)
2. Core Memory                      (~500 tokens)
3. Последнее саммари                (~500 tokens)
4. Последние N сообщений            (~3000 tokens)
5. Доступные tools/skills           (~500 tokens)
```

Стало:

```
1. System prompt                    (~500 tokens)
2. Core Memory                      (~500 tokens)
3. Последнее саммари                (~500 tokens)
4. Auto-RAG результаты              (~400 tokens, 0 если сегмент < window)
   - Блок: "Из ранее в этом разговоре:"
   - Релевантные сообщения из истории сегмента
5. Последние N сообщений            (~2600 tokens, бюджет уменьшен на auto-RAG)
6. Доступные tools/skills           (~500 tokens)
```

Auto-RAG занимает часть бюджета sliding window. При коротких сессиях (предохранитель сработал) — весь бюджет уходит на sliding window, как и раньше.

### Конфигурация

```yaml
# engine.json
autoRag:
  enabled: true
  topK: 3                          # макс результатов в контексте
  maxTokens: 400                   # жёсткий бюджет на auto-RAG в контексте
  relevanceThreshold: 0.5          # distance > порога → не включать
  minMessageTokens: 10             # не embed'ить сообщения короче (см. секцию 2)
```

---

## 2. Векторный индекс по сообщениям (`vec_messages`)

### Новая таблица в klaw.db

```sql
-- Векторный индекс по сообщениям (для auto-RAG)
CREATE VIRTUAL TABLE vec_messages USING vec0(
    embedding float[384]
);
```

Связь с таблицей `messages` — через rowid: `vec_messages.rowid` соответствует `messages.rowid`.

### Что индексируется

| role | Индексируется? | Причина |
|------|----------------|---------|
| `user` | ✅ да, если ≥ `minMessageTokens` | Содержит intent пользователя |
| `assistant` | ✅ да, если ≥ `minMessageTokens` и не чистый tool_call | Содержит ответ/вывод агента |
| `tool` | ❌ нет | Промежуточные данные, суть в ответе assistant |
| `system` | ❌ нет | Служебные маркеры |

**Короткие сообщения** ("ок", "понял", "да") не embed'ятся — плохие embedding'и, мусор в результатах поиска. Порог: `minMessageTokens: 10` (конфигурируемый).

### Timing: fire-and-forget

Embed не блокирует отправку ответа пользователю. После записи в `klaw.db`:

```
Сообщение записано в klaw.db
  │
  ├─→ Ответ отправлен пользователю (немедленно)
  │
  └─→ Fire-and-forget: embed → vec_messages (асинхронно)
      Если embed упал — сообщение доступно через FTS,
      vec_messages переиндексируется при klaw reindex
```

### Потребление ресурсов

При 1000 сообщений/день, ~60% индексируются (user + assistant, ≥ 10 токенов):

| Компонент | Расход |
|-----------|--------|
| Embed time | ~600 × 10ms = 6 секунд/день (ONNX на Pi 5) |
| vec_messages storage | ~600 × 384 × 4 bytes = ~900KB/день |
| За год | ~330MB дополнительно к vec_memory |

Приемлемо для Pi 5 с 16GB RAM.

---

## 3. Субагенты: нескользящее окно, без auto-RAG

### Мотивация

Субагент — короткоживущая корутина с конкретной задачей. Его "история" — предыдущие запуски той же scheduled-задачи. Субагенту не нужен RAG по истории — он не ведёт разговор, а выполняет задачу.

### Нескользящее окно

`subagentWindow: 5` означает: загрузить последние 5 **завершённых запусков** этой задачи из `conversations/scheduler_{task_name}/*.jsonl`. Это фиксированное окно — не скользящее, без обрезки, без RAG.

Каждый "запуск" — это блок: запрос + tool calls + финальный ответ. SubagentWindow=5 значит 5 таких блоков, не 5 отдельных сообщений.

```
Субагент "daily-summary" запускается:

Контекст:
┌─────────────────────────────────────────────────────┐
│ 1. System prompt (SOUL/AGENTS — общий)               │
│ 2. Core memory (общая, read-only)                    │
│ 3. Последние 5 запусков (нескользящее окно):         │
│    - Запуск от 2026-02-23: запрос + ответ            │
│    - Запуск от 2026-02-24: запрос + tool calls + ответ│
│    - ...                                              │
│ 4. Текущий запрос                                     │
│ 5. Tools/Skills                                       │
│                                                       │
│ ❌ НЕТ auto-RAG                                      │
│ ❌ НЕТ sliding window (окно фиксированное)           │
└─────────────────────────────────────────────────────┘
```

### Изменение конфигурации

Переименование для ясности:

```yaml
# engine.json
context:
  slidingWindow: 20               # interactive: скользящее окно
  subagentHistory: 5              # субагенты: кол-во предыдущих запусков (нескользящее)
```

---

## 4. Tool call summarization (отложено, план на будущее)

### Статус: не в MVP

В MVP tool calls (`role=tool` и tool_call блоки от assistant) **не индексируются** в `vec_messages`. Суть tool calls покрывается через:

- `role=user` — intent (зачем вызывался tool)
- `role=assistant` (финальный ответ) — результат (что tool нашёл/сделал)
- FTS по `messages_fts` — точный поиск по сырому содержимому если нужно

### План при обнаружении дыр

Если в реальном использовании обнаружатся кейсы где intent + финальный ответ недостаточны — batch-суммаризация:

```
Триггер: фоновая суммаризация (каждые 50 сообщений)
  │
  ▼
Собрать все tool calls за период, не покрытые предыдущим батчем
  │
  ▼
Один вызов к дешёвой модели (routing.tasks.summarization):
  │
  │  "Сожми эти tool calls в краткие описания (1-2 предложения),
  │   сохраняя что было сделано и ключевые данные из результата:
  │
  │   1. host_exec("systemctl status nginx") → active, running
  │   2. code_execute(python, "pd.read_csv...") → таблица 50 строк
  │   3. file_write("/workspace/report.md") → OK"
  │
  ▼
Саммари каждого tool call → embed → vec_messages
```

Один LLM-вызов на 5-10 tool calls. Совмещается с ритмом фоновой суммаризации — одна задача, одно расписание.

---

## 5. Изменения в существующих секциях дизайна

### Секция 3.3 — Сборка контекстного окна

Добавить блок Auto-RAG между саммари и sliding window (см. секцию 1 этого документа).

### Секция 5.3 — klaw.db

Добавить таблицу:

```sql
-- Векторный индекс по сообщениям (для auto-RAG)
CREATE VIRTUAL TABLE vec_messages USING vec0(
    embedding float[384]
);
```

### Секция 9.2 — engine.json

Добавить секцию `autoRag` (см. конфигурацию в секции 1).

Переименовать `context.subagentWindow` → `context.subagentHistory` (секция 3).

### Секция 2.4 — Субагентная модель

Обновить диаграмму контекста субагента:

```
Субагент (heartbeat):
┌────────────────────────────────┐
│ System prompt (SOUL/AGENTS)    │
│ Core memory (read-only)        │
│ Последние N запусков           │ ← нескользящее, фиксированное
│ Модель из задачи/конфига       │
│ Tools/Skills                   │
│                                │
│ ❌ auto-RAG                   │
│ ❌ sliding window             │
└────────────────────────────────┘
```

### Секция 10 — Оценка объёма работ

| Компонент | Строки (оценка) | Приоритет |
|-----------|-----------------|-----------|
| Auto-RAG: vec_messages индексация + поиск + фильтрация | ~250 | P0 |
| Auto-RAG: интеграция в сборку контекста + предохранитель | ~100 | P0 |
| Субагенты: нескользящее окно (загрузка предыдущих запусков) | ~100 | P0 |

### Секция 12 — Риски

| Риск | Вероятность | Влияние | Митигация |
|------|-------------|---------|-----------|
| Auto-RAG подтягивает нерелевантные результаты, загрязняя контекст | Средняя | Среднее | Relevance threshold (distance > 0.5 → не включать). Конфигурируемый `autoRag.enabled: false` для отключения |
| vec_messages растёт быстро (~330MB/год) | Низкая | Низкое | Для масштаба персонального агента — приемлемо. При необходимости — ротация старых embedding'ов |
| Tool call summarization потребуется раньше чем ожидалось | Средняя | Низкое | Batch-суммаризация готова как план, реализация ~200 строк при необходимости |
